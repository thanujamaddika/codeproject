{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=['This is the first document.',\n",
    "        'This document is the second document.',\n",
    "        'And this is the third one.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer()\n",
    "X=vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "[[0.         0.46941728 0.61722732 0.3645444  0.         0.\n",
      "  0.3645444  0.         0.3645444 ]\n",
      " [0.         0.7284449  0.         0.28285122 0.         0.47890875\n",
      "  0.28285122 0.         0.28285122]\n",
      " [0.49711994 0.         0.         0.29360705 0.49711994 0.\n",
      "  0.29360705 0.49711994 0.29360705]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       textID                                               text  \\\n",
      "0  cb774db0d1                I`d have responded, if I were going   \n",
      "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
      "2  088c60f138                          my boss is bullying me...   \n",
      "3  9642c003ef                     what interview! leave me alone   \n",
      "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
      "\n",
      "                         selected_text sentiment  \n",
      "0  I`d have responded, if I were going   neutral  \n",
      "1                             Sooo SAD  negative  \n",
      "2                          bullying me  negative  \n",
      "3                       leave me alone  negative  \n",
      "4                        Sons of ****,  negative  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV dataset into a pandas DataFrame\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\thanu\\\\OneDrive\\\\Desktop\\\\train.csv\")\n",
    "\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10' '100' '11' '12' '15' '1st' '20' '25' '2day' '30' '4th' 'able'\n",
      " 'about' 'absolutely' 'account' 'actually' 'add' 'after' 'afternoon'\n",
      " 'again' 'ago' 'agree' 'ah' 'ahh' 'ahhh' 'aint' 'air' 'album' 'all'\n",
      " 'almost' 'alone' 'along' 'already' 'alright' 'also' 'although' 'always'\n",
      " 'am' 'amazing' 'an' 'and' 'another' 'answer' 'any' 'anymore' 'anyone'\n",
      " 'anything' 'anyway' 'app' 'apparently' 'apple' 'appreciate' 'are' 'aren'\n",
      " 'around' 'as' 'ask' 'asleep' 'at' 'ate' 'aw' 'awake' 'away' 'awesome'\n",
      " 'aww' 'awww' 'babe' 'baby' 'back' 'bad' 'band' 'bank' 'bbq' 'bday' 'be'\n",
      " 'beach' 'beat' 'beautiful' 'because' 'bed' 'been' 'beer' 'before'\n",
      " 'behind' 'being' 'believe' 'best' 'bet' 'better' 'bgt' 'big' 'bike'\n",
      " 'birthday' 'bit' 'black' 'bless' 'blip' 'blog' 'blue' 'body' 'boo' 'book'\n",
      " 'bored' 'boring' 'both' 'bought' 'bout' 'boy' 'boyfriend' 'boys' 'break'\n",
      " 'breakfast' 'bring' 'bro' 'broke' 'broken' 'brother' 'brothers' 'btw'\n",
      " 'bus' 'business' 'busy' 'but' 'buy' 'by' 'bye' 'cake' 'call' 'called'\n",
      " 'came' 'camera' 'can' 'cannot' 'cant' 'car' 'card' 'care' 'case' 'cat'\n",
      " 'catch' 'cause' 'cd' 'chance' 'change' 'chat' 'check' 'cheese' 'chicken'\n",
      " 'chillin' 'chocolate' 'church' 'city' 'class' 'clean' 'cleaning' 'close'\n",
      " 'closed' 'clothes' 'club' 'coffee' 'cold' 'college' 'com' 'come' 'comes'\n",
      " 'coming' 'comment' 'company' 'completely' 'computer' 'concert' 'congrats'\n",
      " 'cool' 'cos' 'could' 'couldn' 'couple' 'course' 'crazy' 'cream' 'cry'\n",
      " 'crying' 'cup' 'cut' 'cute' 'cuz' 'da' 'dad' 'dance' 'dang' 'dark' 'date'\n",
      " 'david' 'day' 'days' 'dead' 'deal' 'dear' 'decided' 'definitely' 'did'\n",
      " 'didn' 'didnt' 'die' 'died' 'different' 'dinner' 'dm' 'do' 'does' 'doesn'\n",
      " 'doesnt' 'dog' 'doing' 'don' 'done' 'dont' 'down' 'dream' 'dreams'\n",
      " 'dress' 'drink' 'drinking' 'drive' 'driving' 'drunk' 'dude' 'due'\n",
      " 'during' 'dvd' 'each' 'earlier' 'early' 'easy' 'eat' 'eating' 'either'\n",
      " 'else' 'em' 'email' 'end' 'english' 'enjoy' 'enjoying' 'enough' 'episode'\n",
      " 'especially' 'etc' 'even' 'evening' 'ever' 'every' 'everybody' 'everyone'\n",
      " 'everything' 'exactly' 'exam' 'exams' 'except' 'excited' 'exciting'\n",
      " 'eyes' 'face' 'facebook' 'fact' 'fail' 'fair' 'fall' 'family' 'fan'\n",
      " 'fans' 'fantastic' 'far' 'fast' 'favorite' 'fb' 'feel' 'feeling' 'feels'\n",
      " 'feet' 'fell' 'felt' 'few' 'ff' 'figure' 'film' 'final' 'finally' 'find'\n",
      " 'fine' 'fingers' 'finish' 'finished' 'first' 'fix' 'flight' 'flu' 'fly'\n",
      " 'fm' 'follow' 'followers' 'following' 'food' 'for' 'forever' 'forget'\n",
      " 'forgot' 'forward' 'found' 'free' 'french' 'friday' 'friend' 'friends'\n",
      " 'from' 'full' 'fun' 'funny' 'game' 'garden' 'gave' 'gd' 'get' 'gets'\n",
      " 'getting' 'gift' 'girl' 'girls' 'give' 'glad' 'go' 'god' 'goes' 'goin'\n",
      " 'going' 'gone' 'gonna' 'good' 'goodnight' 'google' 'got' 'gotta'\n",
      " 'graduation' 'great' 'green' 'group' 'guess' 'guitar' 'gutted' 'guy'\n",
      " 'guys' 'gym' 'ha' 'had' 'haha' 'hahah' 'hahaha' 'hair' 'half' 'hand'\n",
      " 'hang' 'hanging' 'happen' 'happened' 'happens' 'happy' 'hard' 'has'\n",
      " 'hate' 'hates' 'have' 'haven' 'havent' 'having' 'he' 'head' 'headache'\n",
      " 'heading' 'hear' 'heard' 'heart' 'hehe' 'hell' 'hello' 'help' 'her'\n",
      " 'here' 'hey' 'hi' 'high' 'him' 'his' 'hit' 'hmm' 'holiday' 'home'\n",
      " 'homework' 'hope' 'hopefully' 'hoping' 'horrible' 'hospital' 'hot' 'hour'\n",
      " 'hours' 'house' 'how' 'http' 'hubby' 'hug' 'huge' 'hugs' 'huh' 'hun'\n",
      " 'hungry' 'hurt' 'hurts' 'ice' 'id' 'idea' 'idk' 'if' 'ill' 'im' 'in'\n",
      " 'inside' 'instead' 'interesting' 'internet' 'into' 'iphone' 'ipod' 'is'\n",
      " 'isn' 'isnt' 'it' 'its' 'ive' 'iï' 'jealous' 'job' 'john' 'join' 'jonas'\n",
      " 'july' 'june' 'just' 'justin' 'keep' 'keeps' 'kid' 'kids' 'kind' 'kinda'\n",
      " 'knew' 'know' 'la' 'lady' 'lame' 'laptop' 'last' 'late' 'later' 'lazy'\n",
      " 'learn' 'least' 'leave' 'leaving' 'left' 'legs' 'less' 'let' 'lets'\n",
      " 'life' 'like' 'liked' 'lil' 'line' 'link' 'list' 'listen' 'listening'\n",
      " 'little' 'live' 'living' 'll' 'lmao' 'lol' 'london' 'lonely' 'long'\n",
      " 'longer' 'look' 'looked' 'looking' 'looks' 'lost' 'lot' 'lots' 'love'\n",
      " 'loved' 'lovely' 'loves' 'loving' 'low' 'luck' 'lucky' 'lunch' 'luv' 'ly'\n",
      " 'ma' 'mac' 'mad' 'made' 'mail' 'make' 'makes' 'making' 'mama' 'man'\n",
      " 'many' 'matter' 'may' 'maybe' 'me' 'mean' 'means' 'meant' 'meet'\n",
      " 'meeting' 'men' 'message' 'messages' 'met' 'middle' 'might' 'miles' 'min'\n",
      " 'mind' 'mine' 'minutes' 'miss' 'missed' 'missing' 'mobile' 'mom' 'moment'\n",
      " 'mommy' 'moms' 'monday' 'money' 'month' 'months' 'mood' 'moon' 'more'\n",
      " 'morning' 'most' 'mother' 'mothers' 'move' 'moved' 'movie' 'movies'\n",
      " 'moving' 'mr' 'much' 'mum' 'music' 'must' 'my' 'myself' 'myspace' 'name'\n",
      " 'nap' 'near' 'need' 'needed' 'needs' 'net' 'never' 'new' 'news' 'next'\n",
      " 'nice' 'night' 'nite' 'no' 'non' 'none' 'nope' 'not' 'nothing' 'now'\n",
      " 'number' 'of' 'off' 'office' 'officially' 'oh' 'ohh' 'ok' 'okay' 'old'\n",
      " 'omg' 'on' 'once' 'one' 'ones' 'online' 'only' 'ooh' 'open' 'or' 'other'\n",
      " 'others' 'ouch' 'our' 'out' 'outside' 'over' 'own' 'packing' 'page'\n",
      " 'pain' 'paper' 'parents' 'park' 'part' 'party' 'pass' 'past' 'people'\n",
      " 'perfect' 'person' 'phone' 'photo' 'photos' 'pic' 'pick' 'pics' 'picture'\n",
      " 'pictures' 'pizza' 'place' 'plan' 'plans' 'play' 'played' 'playing'\n",
      " 'please' 'plurk' 'plus' 'point' 'pool' 'poor' 'post' 'posted' 'ppl'\n",
      " 'pretty' 'probably' 'problem' 'problems' 'profile' 'project' 'prom' 'put'\n",
      " 'quick' 'quiet' 'quite' 'radio' 'rain' 'raining' 'rainy' 'ran' 'rather'\n",
      " 're' 'read' 'reading' 'ready' 'real' 'realized' 'really' 'reason' 'red'\n",
      " 'relaxing' 'remember' 'reply' 'rest' 'revision' 'ride' 'right' 'rock'\n",
      " 'room' 'round' 'run' 'running' 'sad' 'sadly' 'safe' 'said' 'same' 'sat'\n",
      " 'saturday' 'save' 'saw' 'say' 'saying' 'says' 'school' 'screen' 'season'\n",
      " 'second' 'see' 'seeing' 'seem' 'seems' 'seen' 'send' 'sent' 'seriously'\n",
      " 'set' 'shall' 'shame' 'share' 'she' 'shirt' 'shoes' 'shop' 'shopping'\n",
      " 'short' 'should' 'show' 'shower' 'shows' 'sick' 'side' 'sigh' 'sign'\n",
      " 'since' 'sing' 'single' 'sister' 'site' 'sitting' 'sleep' 'sleeping'\n",
      " 'sleepy' 'slept' 'slow' 'small' 'smile' 'snl' 'so' 'sold' 'some'\n",
      " 'someone' 'something' 'sometimes' 'somewhere' 'son' 'song' 'songs' 'soo'\n",
      " 'soon' 'sooo' 'soooo' 'sore' 'sorry' 'sound' 'sounds' 'special' 'spend'\n",
      " 'spent' 'star' 'start' 'started' 'starting' 'starts' 'stay' 'staying'\n",
      " 'still' 'stomach' 'stop' 'stopped' 'store' 'story' 'stuck' 'study'\n",
      " 'studying' 'stuff' 'stupid' 'such' 'suck' 'sucks' 'summer' 'sun' 'sunday'\n",
      " 'sunny' 'sunshine' 'super' 'support' 'supposed' 'sure' 'sweet' 'swine'\n",
      " 'take' 'taken' 'takes' 'taking' 'talk' 'talking' 'tea' 'team' 'tell'\n",
      " 'test' 'text' 'than' 'thank' 'thanks' 'that' 'thats' 'the' 'their' 'them'\n",
      " 'then' 'there' 'these' 'they' 'thing' 'things' 'think' 'thinking'\n",
      " 'thinks' 'this' 'tho' 'those' 'though' 'thought' 'three' 'throat'\n",
      " 'through' 'thx' 'tickets' 'til' 'till' 'time' 'times' 'tinyurl' 'tired'\n",
      " 'to' 'today' 'together' 'told' 'tom' 'tomorrow' 'tonight' 'too' 'took'\n",
      " 'top' 'totally' 'tour' 'town' 'traffic' 'train' 'trek' 'tried' 'trip'\n",
      " 'true' 'try' 'trying' 'tuesday' 'turn' 'tv' 'tweet' 'tweeting' 'tweets'\n",
      " 'twitpic' 'twitter' 'two' 'ugh' 'uk' 'under' 'understand' 'unfortunately'\n",
      " 'until' 'up' 'update' 'updates' 'upset' 'ur' 'us' 'use' 'used' 'using'\n",
      " 've' 'vegas' 'version' 'very' 'via' 'video' 'visit' 'vote' 'wait'\n",
      " 'waiting' 'wake' 'walk' 'wanna' 'want' 'wanted' 'wants' 'warm' 'wars'\n",
      " 'was' 'wasn' 'watch' 'watched' 'watching' 'water' 'way' 'we' 'wear'\n",
      " 'weather' 'wedding' 'week' 'weekend' 'weeks' 'weird' 'welcome' 'well'\n",
      " 'went' 'were' 'what' 'whatever' 'whats' 'when' 'where' 'which' 'while'\n",
      " 'white' 'who' 'whole' 'why' 'wife' 'will' 'win' 'windows' 'wine' 'wish'\n",
      " 'wishes' 'wishing' 'with' 'without' 'woke' 'won' 'wonder' 'wonderful'\n",
      " 'wondering' 'wont' 'woo' 'word' 'words' 'work' 'worked' 'working' 'works'\n",
      " 'world' 'worry' 'worse' 'worst' 'worth' 'would' 'wouldn' 'wow' 'write'\n",
      " 'writing' 'wrong' 'wtf' 'www' 'xd' 'xoxo' 'ya' 'yay' 'yea' 'yeah' 'year'\n",
      " 'years' 'yep' 'yes' 'yesterday' 'yet' 'yo' 'you' 'your' 'yours'\n",
      " 'yourself' 'youtube' 'yum' 'yummy' '½m']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "data['text']=data['text'].fillna('')\n",
    "vectorizer=TfidfVectorizer(max_features=1000)\n",
    "X=vectorizer.fit_transform(data['text'])\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
